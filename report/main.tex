% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{array}
\usepackage{booktabs}
\usepackage{float}
\usepackage{footnote}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only


\begin{document}

%%%%%%%%% TITLE
\title{Sign Language Translation from Video to Text}

\author{Marius \textsc{Schmidt-Mengin}\\
	École des Ponts ParisTech\\
	{\tt\small marius.schmidt.mengin@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Clément \textsc{Grisi}\\
École des Ponts ParisTech\\
{\tt\small grisi.clement@gmail.com}
}

\maketitle


\begin{abstract}
	Sign Language Transformers \cite{neccam}, the current state-of-the-art in sign language translation, leverages a Transformer-based encoder-decoder architecture with intermediate gloss supervision to jointly learn continuous sign language recognition and translaton. We investigated two avenues for improving the results presented in \cite{neccam}. First, we slightly modified the Sign Language Transformers pipeline by replacing the 2D CNN with a 3D CNN, better suited for processing videos. Then, we trained the Transformer-based model with additional human keypoint inputs. Although our experiments weren't conclusive, we have laid the foundation for future work and strongly believe their is room for improvement. 
\end{abstract}

\section{Introduction}
According to the World Health Organization, over 5\% of the world’s population -- around 466 million people – are deaf or suffer from disabling hearing loss \cite{WHO}. For many of these individuals, sign language is their primary mean of communication. Despite widespread misconceptions, sign language has its own set of linguistic rules and does not translate the spoken languages word by word. Hence, there is a need for systems to translate sign languages to spoken languages.

\subsection{Sign Language Translation}

Sign language translation is a very challenging task and has been an active field of
research for the past two decades. Most of the research conducted to date has focused on sign language recognition (SLR), that is recognizing the sequence of sign glosses \footnote{sign glosses are spoken language words matching the meaning of signs}, ignoring the linguistic properties of the sign language and assuming that there is a one-to-one mapping to spoken words. Recent advances have tried to tackle sign language translation (SLT), that is the full mapping from signs to spoken words.
\\
explain Sign2Text, Gloss2Text, Sign2Gloss2Text, Sign2(Gloss+Text)
explain dataset
\subsection{Sign Language Transformers}
Here we briefly present the Sign Language Transformers model introduced in \cite{neccam}. We adopt the same notations as in the paper. First, embeddings are extracted from each video frame using a 2D CNN
$$f_{t}= \text{SpatialEmbedding} \left(I_{t}\right) \forall t\in[1, T]$$
This sequence of embeddings is then passed through a Transformer encoder, producing a sequence of encoded spatial embeddings that are then mapped to glosses with a linear layer for sign language recognition. As the number of spatial embeddings may be different from the number of gloss tokens, the Connectionist Temporal Classification (CTC) loss is used. The output of the encoder (before the gloss prediction layer) is passed to the transformer decoder. The decoder also takes as input the target sentence, prepended with a Start Of Sequence (SOS) token, and is trained as in a classical autoregressive seq2seq model. Therefore, Sign Language Transformers learn to predict glosses and sentences jointly.


%-------------------------------------------------------------------------
\section{Video Representations}
\subsection{Spatial Embeddings}

In \cite{neccam}, the spatial embeddings are extracted from a pre-trained 2D CNN. Notably, the 2D CNN they use for their experiments is pre-trained on sign language video \cite{hmm}. The authors of \cite{neccam} showed that this is crucial to achieve good performance, as embeddings extrated from an EfficientNet \cite{effnet} pre-trained on ImageNet are substentially worse on the Sign2Gloss task (the reported WER is almost halved).
It is worth noting that this 2D CNN feature extractor is not trained jointly with the Sign Language Transformer.
\subsection{Spatio-Temporal Embeddings}
We investigate replacing the 2D CNN by a 3D CNN. Our goal is to pre-train this 3D CNN to extract good representations of sign language videos for the Sign2Text task. Training it end-to-end with the Sign Language Transformer is difficult because this requires a lot of GPU memory. Indeed, the input videos can have many frames (up to 475 in the PHOENIX-2014T \cite{phoenix} dataset) and cannot easily be split into multiple part since they are paired with a sentence. Although one possibility would be to reduce the framerate, we prefer to experiment with techniques that do not require textual targets. We experiment with several self-supervised pre-training tasks.
\subsection{Self-Supervised Learning of Video Representations}
Recently, there has been a growing interest in self-supervised learning for image, video, audio or text data. Self-supervised learning allows to leverage large quantities of unlabeled data. Multiple approaches have been explored for self-supervised learning from videos. We explore two of them: predicting whether a video is played forward or backward \cite{arrow} and predicting the order of shuffled clips \cite{vcop}. Other approaches include learning from temporaly modified videos \cite{playback-rate, pace, temp-trans}, learning to predict the future \cite{pred-coding} and contrastive learning \cite{bert-video}.
\subsubsection{Learning from the Arrow of Time}
It has been shown \cite{arrow} that learning to predict if a video clip is played forward or backward can be useful to learn good representations of videos. Our intuition tells us that this task may be particularly well adapted for learning high-level representations of sign language videos since it is very hard to solve for humans that do not know sign language.

\subsubsection{Predicting the Order of Shuffled Clips}
The goal of this task is to predict the order of different clips in a video. More specifically, given some video, several (in general, 3) non-overlapping clips are extracted. A random permutation is chosen (in the case of 3 clips, there are 6 possible permutations). Each of them is then passed through the 3D CNN and average-pooled to a feature vector. The feature vectors of all the clips are concatenated and passed through a 2-layer MLP-like neural network, which predicts the permutation of the clips. Intuitively, only high-level features allow to predict the order of non-overlapping clips.
\subsection{Human Keypoint Estimation}
\subsubsection{Modelling the Body Pose}
Sign language being visual, signers rely on multiple complementaty channels to convey information. These can be grouped into two main categories: manual and non-manual features. Manual features mainly correspond to the hand shape and its motion through time. Although these are often considered as the governing part of the sign morphology, they alone do not enclose the full context of the conveyed information. To give clarity, add precisions and nuance their words, signers use non-manual features, such as facial expressions, mouth gesture and body pose. This motivates our work to investigate whether explicitly modelling the body pose (hands, face, upper body) could benefit neural sign language translation.
\subsubsection{Body Keypoints as Additional Inputs}
In order to extract 2D and 3D body keypoints, we applied a pre-trained version of DOPE \cite{dope} to each frame in the PHOENIX-2014T dataset (Figure \ref{fig:2d_3d}). We slighltly modified the code provided by the authors by adding three pooling layers on top of the ResNet50 backbone (average pooling, max pooling, argmax pooling). This allowed us to extract 2D and 3D whole-body keypoints (face, hands, body) as well as three pooling embeddings for each frame of the PHOENIX-2014T dataset. The obtained human keypoints were normalized by the mean and standard deviation of the corresponding body part and used as additional input to our sign language transformer model, together with the pooling embeddings.

\begin{figure}[h]
	\begin{subfigure}[h]{0.5\linewidth}
		\centering
			\includegraphics[width=4cm]{fig/toy_DOPE_v1_0_0_2d.png}
	\end{subfigure}\hfill
	\begin{subfigure}[]{0.5\linewidth}
		\centering
		\includegraphics[width=4cm]{fig/toy_DOPE_v1_0_0_3d.png}
	\end{subfigure}
	\caption{2D \& 3D Keypoints on PHOENIX-2014T}
	\label{fig:2d_3d}
\end{figure}

\section{Experiments}
\subsection{Implementation}

To have more flexibility, we re-implement\footnote{\url{https://github.com/marius-sm/sign_language_translation}} Sign Language Transformers \cite{neccam} using Pytorch and HuggingFace transformers \cite{huggingface}. In all our experiments, we use a R(2+1)D \cite{r2plus1} pre-trained on the Kinetics-400 \cite{kinetics} dataset from the Torchvision library as our 3D feature extractor. The last convolutional layer produces 512 channels. Whenever necessary, we average-pool those channels to produce embeddings.

To learn the arrow of time, we use clips of 8 frames and train with a batch size of 32. For clip order prediction, we extract 8 clips from each video with intervals of 8 frames and train with a batch size of 8. We optimize both networks with Adam and a low learning rate of $10^{-5}$ to avoid destroying the Kinetics-400 features.

Once trained, we use these networks to extract the embeddings from all videos. As the dataset videos have significantly more than 8 frames,  extracting the embeddings of a video in a single pass results in a strong train/test discrepancy. To adress this issue, we instead subdivide each video in chunks of 8 frames and extract the embedding of each chunk independently.
\subsection{Qualitative assessments of the embeddings}
After pooling the feature along spatial dimensions, we obtain a sequence of spatial embeddings
$$f_t, t\in[1, ..., \tilde{T}]$$
where $\tilde{T}\leq T$ ($T$ is the number of frames of the input video). For a 2D CNN, $T=\tilde{T}$ whereas R(2+1)D has a temporal stride of $8$, resulting in $\tilde{T} = \lceil T/8 \rceil$.
To assess the quality of our embeddings, we visualize their cosine similarities. Given two videos $v^1$ and $v^2$ and their spatial embeddings sequences $f_t^1, t\in\{1, ..., \tilde{T}^1\}$ and $f_u^2, u\in \{1, ...,  \tilde{T}^2\}$, we compute the matrices $A$ and $B$:
$$A_{tu} = \frac{{f_t^1}^\intercal f_{t'}^1}{\lVert f_t^1 \rVert \lVert f_{t'}^1 \rVert}, \quad\forall t, t' \in\{1, ..., \tilde{T}^1\}$$
$$B_{tu} = \frac{{f_t^1}^\intercal f_u^2}{\lVert f_t^1 \rVert \lVert f_u^2 \rVert}, \quad \forall t \in\{1, ..., \tilde{T}^1\}, u \in\{1, ..., \tilde{T}^2\}$$
That is, we compare the embeddings of a given video with the other embeddings of the same video (matrix $A$) and with the embeddings of another video (matrix $B$). Intuitively, we want 
\begin{itemize}
	\item matrix $A$ to look like a diagonal matrix, where temporaly close frames have similar embeddings, but frames that are far appart have disimilar embeddings, so that there is enough information to recover the translation
	\item matrix $B$ to not have many values close to $1$ as two different videos probably don't translate to related sentences.
\end{itemize}

We first visualize the embeddings of a the 2D CNN pre-trained on sign language data.
\subsection{Influence of Body Keypoints}

We summarize hereunder the translation performances observed on PHOENIX-2014T dev dataset when feeding the network additionnal body keypoints inputs. Table \ref{tab:embeddings} shows the performances when concatenating the 2D CNN embeddings from \cite{neccam} with the pooling embeddings extracted from each video frame with DOPE.
\begin{table}[H]
\centering
\begin{tabular}{*5c}
	\toprule
	Input &  \multicolumn{4}{c}{DEV}\\
	\midrule
	{}   & BLEU-1   & BLEU-2    & BLEU-3   & BLEU-4\\
	Neccam   &  45.54 & 32.60  & \textbf{25.30}  & \textbf{20.69}\\
	+avg   &  46.61 & 33.17   & 25.21  & 20.09\\
	+max   &  45.98  &  32.44   & 24.79  & 19.99\\
	+amax  &  \textbf{47.31} &  \textbf{33.34}   & 25.07  & 19.81\\
	+max+amax  &  44.32 & 31.50  & 24.09  & 19.27\\
	\bottomrule
\end{tabular}
\caption{Sign2Text with additional Pooling Embeddings}
\label{tab:embeddings}
\end{table}
Table \ref{tab:2d_keypoints} (resp. Table \ref{tab:3d_keypoints}) shows the performances of the five best experiments obtained when concatenating the same 2D CNN embeddings with the 2D (resp. 3D) body keypioints extracted using DOPE. While the 2D keypoints had to be normalized by the mean and standard deviation of the corresponding body part, the 3D keypoints were already between $0$ and $1$ hence didn't need to be normalized. 
\begin{table}[h]
	\centering
	\begin{tabular}{*5c}
		\toprule
		Input &  \multicolumn{4}{c}{DEV}\\
		\midrule
		{}   & BLEU-1   & BLEU-2    & BLEU-3   & BLEU-4\\
		Neccam   &  45.54 & \textbf{32.60}  & \textbf{25.30}  & \textbf{20.69}\\
		+hand\_2d   &  45.66 & 32.52   & 24.79  & 19.18\\
		+face\_2d   &  46.06  &  32.44   & 24.63  & 19.61\\
		+body\_2d   &  45.73 &  32.04   & 24.34  & 19.53\\	
		+fh\_2d\footnotemark[3]  &  45.53 &  32.21  & 24.31  & 19.34\\	
		+fhb\_2d\footnotemark[4] &  \textbf{46.09} &  32.16   & 23.96  & 18.75\\	
		\bottomrule
	\end{tabular}
	\caption{Sign2Text with additional 2D Keypoints}
	\label{tab:2d_keypoints}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{*5c}
		\toprule
		Input &  \multicolumn{4}{c}{DEV}\\
		\midrule
		{}   & BLEU-1   & BLEU-2    & BLEU-3   & BLEU-4\\
		Neccam   &  45.54 & 32.60  & \textbf{25.30}  & \textbf{20.69}\\
		+hand\_3d   &  46.19 & 32.49   & 24.24  & 18.92\\
		+face\_3d   &  45.56  &  32.10 & 24.27  & 19.27\\
		+body\_3d   &  44.96 &  31.96   & 24.32  & 19.46\\	
		+fh\_3d\footnotemark[3]  &  \textbf{46.37} &  \textbf{32.83}   & 24.88  & 19.72\\	
		+fhb\_3d\footnotemark[4]  &  46.09 &  32.34   & 24.00  & 18.85\\	
		\bottomrule
	\end{tabular}
	\caption{Sign2Text with additional 3D Keypoints}
	\label{tab:3d_keypoints}
\end{table}
\footnotetext[3]{face+hand keypoints} 
\footnotetext[4]{face+hand+body keypoints}

No matter what combination of additional inputs we fed the network, we didn't witness any significative improvement in translation performances. This is mainly due to DOPE performing relatively poorly on the PHOENIX-2014T dev dataset. Even though it manages to identify body keypoints in almost every single frames, it only rarely detects the signer's face and hands (Figure \ref{fig:pie_charts}). Hence, we're mostly adding noise to the 2D CNN embeddings. Instead of using a single model for whole-body pose estimation, one could try using indepedant hand, face and body experts to detect more precisely each part of the signer's expression.

\begin{figure}[h]
	\centering
	\includegraphics[width=8.7cm]{fig/keypoints.pdf}
	\caption{DOPE Results on  PHOENIX-2014T}
	\label{fig:pie_charts}
\end{figure} 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
